bluehat: Python Processing of Spectral Imagery

I DON'T LIKE THE NAME

  PyPSI -- how do you pronounce it?
           "Pip-see"  or "Pie-Sigh" (pip-squeak?)
	   also, it's hard to type! (I keep doing psypi)
  psiutil -- clear that these are utilities
             leaves open other psi components (psifindtarget)
  msiutil --	     	    
  SIPPy -- re-arraning the letters; too close to SPy?
  BlueHat -- obscure reference to one of the algorithms,
             but does't /really/ mean anything, and is
	     just meant to be unique and easy to remember
	     and easy to pronounce (ie a "brand"!)
	     [Blue Hyperspectral Analysis Tools]
	     [Best Let U Experiment: ...]
  BUSI -- Basic Utilites for Spectral Imagery
  SUSIE -- Some Utilites for Spectral Image Exploitation
  SUSIEQ -- ... and Quantification
  sext -- spectral exploitation tools
  DTACSI -- Detection of Targets, Anomalies, and Changes in Spectral Imagery
  BUDTAAC -- Basic Util for Detecting Targets, Anomalies, and Anomlaous Changes
  LostWrangler -- both a reference to our trail system, and a nod to
                  how confusing it is to wrangle data
  ONAH -- of needles and haystacks
  MHSIP -- multi/hyper SIP
  LUSI/GUSI -- Local/Global utilites for spectral imagery
  GUTSI -- General Utilites and Tools for Spectral Imagery


VISION

Grande Vision is an all encompassing multi-author open-source package that does all kinds of spectral processing, focused perhaps on target detection (less emphasis on endmembers, segmentation, etc -- though may include basic algorithms if they are useful for target detection), but including gas detection, solid target detection, anomaly detection, and anomalous change detection.  In this grander vision, machine learning might play a sizeable role (eg, with tools for matched pair machine learning).

Nearer term vision is a kind of 'util' directory that enables different target detecton projects to use common 'standard' processing routines.  (I'm thinking: whitening, various matched filters, ROC utilities, etc)  The minimial aim here is just to avoid all that cutting-and-pasting, and copying and overwriting, and non-propagating updates that are inevitable when the projects are fully independent.

OPEN QUESTIONS

The main frustration with this notion of common standards is the interleave issue.  We can be sure that our data kits identify which interleave strategy is employed, but for intermediately processed 2d arrays (which I see as the central data structure for most of the algorithms I am contemplating), is it the first or the second dimension that is spectral. Efficiency and (some) convention seems to prefer the second, but the first is more consistent with how the mathematics is written

An overall design consideration is the extent to which the package is 'matlab-like' in terms of providing basic utilities with generic interfaces, or more 'pythonic' with data structures that take advantage of python's OOP style.  For instance, should a data matrix "know" which dimension is its spectral dimension?

Although I don't "like" SPy (Spectral Python), I do use it (for reading ENVI files, mostly), and its goals are very similar to mine, and some level of interoperability would be a virtue. In fact, one might argue that I should be writing routines "in SPy", perhaps by contributing my algorithms to the SPy project.  So what don't I like about SPy? 1/ it's not mine (ie, not invented here); 2/ I don't actually know who maintains it; 3/ I don't like the fact that it requires me to use its idea of data structures -- so if I start writing "in" SPy, I'll be stuck "with" SPy.

DATA KITS

A related (possibly integrated) vision is to develop "data kits" -- data files in some standard format, with the data cube of interest along with auxiliary data (eg, target signature, ground truth) to enable algorithms to be tested (do they work?) and evaluated (how well do they work?).  Partly this is a community service -- not only should we not force newcomers to re-wrangle the data, we should enable their results to be compared with others.  Some data wrangling is just tedious, and there's no good reason to raise the barrier to entry just for its own sake; other data wrangling is more interesting (eg, band selection) and requires the kind of scientific judgement that one would like to promote.  I think comparision of algorithms is valuable, but I'm not a fan of leaderboards, at least not formally. Getting back to the wrangling issue; is your new robust-to-outliers matched-filter algorithm better because it is robust to outliers, or because you made better data wrangling decisions?

Keywords for a data kit
  cube
    interleave
    lines
    samples
    bands
  target
  truth
    lines
    samples
  lambda (for wavelengths)
  ...
  origin
  citation
  



SOFTWARE COMPONENTS

totally generic utilities:
 verbose
 breakpipe

whiten object
  covariance estimation, regularization

covariance estimation
  via SMT

roc curve utilities

interleave-aware cube flattening

stripes for insample/outsample

basic hdf reading utilities

ABC structure for ecftmf?


---

2022-05-30

Maybe we want a more heterogeneous architecture; eg
HOME/src/PKG/src/rocu.py
                 verbose.py
		 ...

and then put HOME/src/PKG/src on the PYTHONPATH
so then 'import rocu' works as if it were in local directory

this avoids the problem of multiple copies of rocu, verbose, etc laying around

but...there is no sense of namespace, so...
still with:
HOME/src/PKG/src/rocu.py
                 verbose.py
		 ...

but also
HOME/src/PKG/__init__.py
with lines like:
   from .src import rocu
   form .src import verbose

and now HOME/src on the PYTHONPATH
and then in the calling routine, you can use

from pypsi import rocu

or

import pypsi

followed by

    ... = pypsi.rocu.roc(...)

but that env variable seems too loose; so what
if you set it up so that

HOME/src/PKG/PKG/...

and included HOME/src/PKG on the path??

Or...how about

PYTHONPATH = :$HOME/src/pyutil  in the zshrc

and move psipy (i mean pypsi) to be HOME/src/pyutil/psipy

Then...both I and users of the psipy package will have the
same calling conventions; namely
  import pypsi
  ...
  pypsi.rocu.roc(...)

or

  from pypsi import rocu
  rocu.roc(...)

or

  from pypsi.rocu import roc
  roc(...)

5-31:

Am thinking about using 'spectral_axis' instead of 'interleave' mostly
because an integer is more well-defined than a three-letter acronym

interleave -- really defined for 3d cubes
           -- is string, one of: BSQ, BIL, BIP
spectral_axis -- in principle applies to 2d arrays and 3d cubes
              -- sax=0 ==> BSQ which is my preferred default
	      -- sax=1 ==> BIL for 3d, more like BIP for 2d
	      -- sax=2 ==> BIP for 3d, meaningless for 3d
	      -- sax=-1 ==> sax=2 for 3d, sax=1 for 2d, BIP in both cases

How to flatten a BIL file?

What to call the 2d array structure, how about array!? (or 'datarray')

I can see a potential problem.  We have a BIP image and so we set
spectral_axis=-1.  We convert it to a 2d array with first axis the
spectral axis.  But when we go to whiten, we use the spectral_axis
modifier, and end up telling it that the 2d array has spectral axis
last, which is incorrect.

One solution: always /assume/ that 2d arrays have spectral axis first,
regardless of what is specified in the argument list.  --  that means
we need to fix IRR and DRBIG to treat the 2d arrays this way.

Also, it means that whiten should /not/ support a spectral_axis for
its 2d manipulations.  But then again, we should upgrade whiten.py so
use 3d cubes too.

Also, basic.py should probably support some kind of global spectral_axis,
set perhaps when the data file is first read in, or even chosen as the
default based on efficiency or other coding style preferences.

...unless I want to make a data cube that knows for itself which axis
is spectral -- ie, to create a DataCube class.  But if I do create a
DataCube class, I think it should sit "on top of" this more generic
implementation, so that other code (and other folks' code) would have
more options for integrating in.

EXPERIMENT: I've been thinking that for 2d arrays, one convention would
be more efficient than another, but I should actual do some experiments
and see if that's really the case. For moderate sized array, it may not
matter, since transposes are just different "views" and the underlying
computations in C don't matter so much.  Maybe one is faster than another
when it comes to specialized hardware, or for very large datasets?

Estimating covariance; about the same for both.
Pulling out every 100'th pixel, [pixels][bands] is better
PUlling out every 10'th band, about the same.

import numpy as np

x = np.random.rand(1_000_000,100)  ## [pixels][bands]
y = x.T.copy()                     ## [bands][pixels]

%%timeit
C = np.dot(x.T,x)
38.9 ms ± 5.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%%timeit
C = np.dot(y,y.T)
38.5 ms ± 6.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%%timeit
xt = x[::100,:].copy()
1.46 ms ± 1.71 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%%timeit
yt = y[:,::100].copy()
8.42 ms ± 55.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%%timeit
xt = x[:,::10].copy()
76.6 ms ± 751 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%%timeit
yt = y[:,::10].copy()
76.5 ms ± 721 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)




	     
